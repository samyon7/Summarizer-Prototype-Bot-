{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVXJBRhTea32",
        "outputId": "b19167b8-7db2-406a-a37a-5ceeaf637fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "id": "jFpaBdlYebwD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('resume.csv')\n",
        "\n",
        "# Text preprocessing\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "data['isi'] = data['isi'].apply(clean_text)\n",
        "data['Hasil Ringkasan'] = data['Hasil Ringkasan'].apply(clean_text)"
      ],
      "metadata": {
        "id": "oM1pS_RTejEX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(subset=['isi', 'Hasil Ringkasan'])\n",
        "\n",
        "# Define maximum lengths\n",
        "max_article_length = 200\n",
        "max_summary_length = 50"
      ],
      "metadata": {
        "id": "N1FYt7r-f88B"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, texts, max_size=10000):\n",
        "        self.word2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "        self.idx2word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
        "        self.build_vocab(texts, max_size)\n",
        "\n",
        "    def build_vocab(self, texts, max_size):\n",
        "        word_counts = Counter()\n",
        "        for text in texts:\n",
        "            word_counts.update(text.split())\n",
        "        for word, _ in word_counts.most_common(max_size - 4):\n",
        "            idx = len(self.word2idx)\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ],
      "metadata": {
        "id": "7ppinVhmgmXN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = data['isi'].tolist() + data['Hasil Ringkasan'].tolist()\n",
        "vocab = Vocab(texts)"
      ],
      "metadata": {
        "id": "7gS7yucPgs_B"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, vocab, max_length):\n",
        "    tokens = [vocab.word2idx.get(word, vocab.word2idx['<unk>']) for word in text.split()]\n",
        "    tokens = [vocab.word2idx['<sos>']] + tokens + [vocab.word2idx['<eos>']]\n",
        "    if len(tokens) < max_length:\n",
        "        tokens += [vocab.word2idx['<pad>']] * (max_length - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:max_length]\n",
        "    return tokens\n",
        "\n",
        "data['article_tokens'] = data['isi'].apply(lambda x: tokenize(x, vocab, max_article_length))\n",
        "data['summary_tokens'] = data['Hasil Ringkasan'].apply(lambda x: tokenize(x, vocab, max_summary_length))"
      ],
      "metadata": {
        "id": "CtXRg19Ig8Br"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "8WgrNyoRhERs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.data.iloc[idx]['article_tokens']\n",
        "        summary = self.data.iloc[idx]['summary_tokens']\n",
        "        return torch.tensor(article), torch.tensor(summary)"
      ],
      "metadata": {
        "id": "zPcuOuqBhKyj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SummarizationDataset(train_data)\n",
        "val_dataset = SummarizationDataset(val_data)"
      ],
      "metadata": {
        "id": "Xi4KeQGshLlx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "zqqDeiIthQVC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-ZJFpFbhR4Z",
        "outputId": "24ec33ca-a920-4161-b3fd-0491367b0b72"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.SummarizationDataset at 0x7e32bbe5e530>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(file_path, vocab, embedding_dim):\n",
        "    embeddings = np.zeros((len(vocab), embedding_dim))\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in vocab.word2idx:\n",
        "                idx = vocab.word2idx[word]\n",
        "                embeddings[idx] = np.array(vector, dtype=np.float32)\n",
        "    return torch.tensor(embeddings, dtype=torch.float32)\n",
        "\n",
        "embedding_dim = 200\n",
        "glove_embeddings = load_glove_embeddings('/content/vectors_200d.txt', vocab, embedding_dim)"
      ],
      "metadata": {
        "id": "RyNI2lnRhS7M"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_dim, hidden_size, num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embedding_dim, hidden_size, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output)\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        target_vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
        "\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        input = target[:, 0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(input.unsqueeze(1), hidden, cell)\n",
        "            outputs[:, t] = output.squeeze(1)\n",
        "            teacher_force = np.random.rand() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(2)\n",
        "            input = target[:, t] if teacher_force else top1.squeeze(1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "-I3UXnQIhYGT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = len(vocab)\n",
        "hidden_size = 256\n",
        "output_size = len(vocab)\n",
        "\n",
        "encoder = Encoder(input_size, embedding_dim, hidden_size)\n",
        "decoder = Decoder(output_size, embedding_dim, hidden_size)\n",
        "model = Seq2Seq(encoder, decoder, torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "model = model.to(model.device)"
      ],
      "metadata": {
        "id": "PAzoKVZAhb0C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.embedding.weight.data.copy_(glove_embeddings)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx['<pad>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
      ],
      "metadata": {
        "id": "1Q8B3LMUhdBG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for source, target in dataloader:\n",
        "        source, target = source.to(model.device), target.to(model.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(source, target)\n",
        "\n",
        "        output = output[:, 1:].contiguous().view(-1, output.shape[-1])\n",
        "        target = target[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "# Evaluation Loop\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for source, target in dataloader:\n",
        "            source, target = source.to(model.device), target.to(model.device)\n",
        "\n",
        "            output = model(source, target, teacher_forcing_ratio=0)\n",
        "\n",
        "            output = output[:, 1:].contiguous().view(-1, output.shape[-1])\n",
        "            target = target[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "gXGf4Q6Phe8L"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer)\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BmtgGblvhiFV",
        "outputId": "3d13136b-0df5-41da-ca4c-d1e09958faf2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 9.1993, Val Loss: 9.2071\n",
            "Epoch: 2, Train Loss: 9.1995, Val Loss: 9.2067\n",
            "Epoch: 3, Train Loss: 9.1977, Val Loss: 9.2064\n",
            "Epoch: 4, Train Loss: 9.1965, Val Loss: 9.2060\n",
            "Epoch: 5, Train Loss: 9.1967, Val Loss: 9.2056\n",
            "Epoch: 6, Train Loss: 9.1971, Val Loss: 9.2053\n",
            "Epoch: 7, Train Loss: 9.1958, Val Loss: 9.2097\n",
            "Epoch: 8, Train Loss: 9.1954, Val Loss: 9.2096\n",
            "Epoch: 9, Train Loss: 9.1938, Val Loss: 9.2094\n",
            "Epoch: 10, Train Loss: 9.1939, Val Loss: 9.2063\n",
            "Epoch: 11, Train Loss: 9.1935, Val Loss: 9.2064\n",
            "Epoch: 12, Train Loss: 9.1915, Val Loss: 9.2027\n",
            "Epoch: 13, Train Loss: 9.1907, Val Loss: 9.2027\n",
            "Epoch: 14, Train Loss: 9.1902, Val Loss: 9.2020\n",
            "Epoch: 15, Train Loss: 9.1915, Val Loss: 9.2031\n",
            "Epoch: 16, Train Loss: 9.1911, Val Loss: 9.2002\n",
            "Epoch: 17, Train Loss: 9.1893, Val Loss: 9.1985\n",
            "Epoch: 18, Train Loss: 9.1902, Val Loss: 9.1993\n",
            "Epoch: 19, Train Loss: 9.1884, Val Loss: 9.2000\n",
            "Epoch: 20, Train Loss: 9.1870, Val Loss: 9.1999\n",
            "Epoch: 21, Train Loss: 9.1850, Val Loss: 9.1994\n",
            "Epoch: 22, Train Loss: 9.1857, Val Loss: 9.2002\n",
            "Epoch: 23, Train Loss: 9.1854, Val Loss: 9.2000\n",
            "Epoch: 24, Train Loss: 9.1844, Val Loss: 9.1986\n",
            "Epoch: 25, Train Loss: 9.1837, Val Loss: 9.1947\n",
            "Epoch: 26, Train Loss: 9.1852, Val Loss: 9.1936\n",
            "Epoch: 27, Train Loss: 9.1807, Val Loss: 9.1919\n",
            "Epoch: 28, Train Loss: 9.1809, Val Loss: 9.1914\n",
            "Epoch: 29, Train Loss: 9.1799, Val Loss: 9.1913\n",
            "Epoch: 30, Train Loss: 9.1789, Val Loss: 9.1936\n",
            "Epoch: 31, Train Loss: 9.1772, Val Loss: 9.1933\n",
            "Epoch: 32, Train Loss: 9.1772, Val Loss: 9.1925\n",
            "Epoch: 33, Train Loss: 9.1762, Val Loss: 9.1918\n",
            "Epoch: 34, Train Loss: 9.1756, Val Loss: 9.1933\n",
            "Epoch: 35, Train Loss: 9.1753, Val Loss: 9.1927\n",
            "Epoch: 36, Train Loss: 9.1745, Val Loss: 9.1927\n",
            "Epoch: 37, Train Loss: 9.1735, Val Loss: 9.1921\n",
            "Epoch: 38, Train Loss: 9.1724, Val Loss: 9.1901\n",
            "Epoch: 39, Train Loss: 9.1701, Val Loss: 9.1917\n",
            "Epoch: 40, Train Loss: 9.1702, Val Loss: 9.1922\n",
            "Epoch: 41, Train Loss: 9.1679, Val Loss: 9.1897\n",
            "Epoch: 42, Train Loss: 9.1677, Val Loss: 9.1865\n",
            "Epoch: 43, Train Loss: 9.1657, Val Loss: 9.1848\n",
            "Epoch: 44, Train Loss: 9.1667, Val Loss: 9.1833\n",
            "Epoch: 45, Train Loss: 9.1628, Val Loss: 9.1864\n",
            "Epoch: 46, Train Loss: 9.1638, Val Loss: 9.1830\n",
            "Epoch: 47, Train Loss: 9.1595, Val Loss: 9.1839\n",
            "Epoch: 48, Train Loss: 9.1592, Val Loss: 9.1785\n",
            "Epoch: 49, Train Loss: 9.1594, Val Loss: 9.1762\n",
            "Epoch: 50, Train Loss: 9.1575, Val Loss: 9.1723\n",
            "Epoch: 51, Train Loss: 9.1536, Val Loss: 9.1747\n",
            "Epoch: 52, Train Loss: 9.1530, Val Loss: 9.1785\n",
            "Epoch: 53, Train Loss: 9.1510, Val Loss: 9.1769\n",
            "Epoch: 54, Train Loss: 9.1478, Val Loss: 9.1781\n",
            "Epoch: 55, Train Loss: 9.1467, Val Loss: 9.1735\n",
            "Epoch: 56, Train Loss: 9.1446, Val Loss: 9.1705\n",
            "Epoch: 57, Train Loss: 9.1407, Val Loss: 9.1713\n",
            "Epoch: 58, Train Loss: 9.1374, Val Loss: 9.1694\n",
            "Epoch: 59, Train Loss: 9.1372, Val Loss: 9.1660\n",
            "Epoch: 60, Train Loss: 9.1310, Val Loss: 9.1606\n",
            "Epoch: 61, Train Loss: 9.1268, Val Loss: 9.1374\n",
            "Epoch: 62, Train Loss: 9.1201, Val Loss: 9.1103\n",
            "Epoch: 63, Train Loss: 9.1133, Val Loss: 9.0900\n",
            "Epoch: 64, Train Loss: 9.1070, Val Loss: 9.0780\n",
            "Epoch: 65, Train Loss: 9.0945, Val Loss: 9.0579\n",
            "Epoch: 66, Train Loss: 9.0845, Val Loss: 9.0393\n",
            "Epoch: 67, Train Loss: 9.0495, Val Loss: 9.0024\n",
            "Epoch: 68, Train Loss: 9.0303, Val Loss: 8.9281\n",
            "Epoch: 69, Train Loss: 8.9901, Val Loss: 8.8799\n",
            "Epoch: 70, Train Loss: 8.9293, Val Loss: 8.8336\n",
            "Epoch: 71, Train Loss: 8.8921, Val Loss: 8.7836\n",
            "Epoch: 72, Train Loss: 8.8066, Val Loss: 8.7310\n",
            "Epoch: 73, Train Loss: 8.7704, Val Loss: 8.6783\n",
            "Epoch: 74, Train Loss: 8.7060, Val Loss: 8.6268\n",
            "Epoch: 75, Train Loss: 8.6470, Val Loss: 8.5775\n",
            "Epoch: 76, Train Loss: 8.6230, Val Loss: 8.5308\n",
            "Epoch: 77, Train Loss: 8.5675, Val Loss: 8.4862\n",
            "Epoch: 78, Train Loss: 8.5497, Val Loss: 8.4433\n",
            "Epoch: 79, Train Loss: 8.4218, Val Loss: 8.4021\n",
            "Epoch: 80, Train Loss: 8.3634, Val Loss: 8.3618\n",
            "Epoch: 81, Train Loss: 8.2690, Val Loss: 8.3224\n",
            "Epoch: 82, Train Loss: 8.2427, Val Loss: 8.2844\n",
            "Epoch: 83, Train Loss: 8.2099, Val Loss: 8.2480\n",
            "Epoch: 84, Train Loss: 8.1311, Val Loss: 8.2133\n",
            "Epoch: 85, Train Loss: 8.0951, Val Loss: 8.1798\n",
            "Epoch: 86, Train Loss: 8.0710, Val Loss: 8.1482\n",
            "Epoch: 87, Train Loss: 8.0309, Val Loss: 8.1178\n",
            "Epoch: 88, Train Loss: 7.9675, Val Loss: 8.0885\n",
            "Epoch: 89, Train Loss: 7.9450, Val Loss: 8.0604\n",
            "Epoch: 90, Train Loss: 7.8912, Val Loss: 8.0335\n",
            "Epoch: 91, Train Loss: 7.8606, Val Loss: 8.0076\n",
            "Epoch: 92, Train Loss: 7.8173, Val Loss: 7.9827\n",
            "Epoch: 93, Train Loss: 7.7729, Val Loss: 7.9587\n",
            "Epoch: 94, Train Loss: 7.7421, Val Loss: 7.9353\n",
            "Epoch: 95, Train Loss: 7.7160, Val Loss: 7.9129\n",
            "Epoch: 96, Train Loss: 7.6835, Val Loss: 7.8915\n",
            "Epoch: 97, Train Loss: 7.6515, Val Loss: 7.8707\n",
            "Epoch: 98, Train Loss: 7.6207, Val Loss: 7.8506\n",
            "Epoch: 99, Train Loss: 7.5915, Val Loss: 7.8313\n",
            "Epoch: 100, Train Loss: 7.5597, Val Loss: 7.8127\n",
            "Epoch: 101, Train Loss: 7.5268, Val Loss: 7.7948\n",
            "Epoch: 102, Train Loss: 7.5284, Val Loss: 7.7777\n",
            "Epoch: 103, Train Loss: 7.4963, Val Loss: 7.7611\n",
            "Epoch: 104, Train Loss: 7.4549, Val Loss: 7.7452\n",
            "Epoch: 105, Train Loss: 7.4370, Val Loss: 7.7299\n",
            "Epoch: 106, Train Loss: 7.3973, Val Loss: 7.7152\n",
            "Epoch: 107, Train Loss: 7.3875, Val Loss: 7.7009\n",
            "Epoch: 108, Train Loss: 7.3583, Val Loss: 7.6872\n",
            "Epoch: 109, Train Loss: 7.3392, Val Loss: 7.6740\n",
            "Epoch: 110, Train Loss: 7.3296, Val Loss: 7.6614\n",
            "Epoch: 111, Train Loss: 7.3163, Val Loss: 7.6492\n",
            "Epoch: 112, Train Loss: 7.3057, Val Loss: 7.6374\n",
            "Epoch: 113, Train Loss: 7.2835, Val Loss: 7.6262\n",
            "Epoch: 114, Train Loss: 7.2506, Val Loss: 7.6153\n",
            "Epoch: 115, Train Loss: 7.2372, Val Loss: 7.6048\n",
            "Epoch: 116, Train Loss: 7.2269, Val Loss: 7.5947\n",
            "Epoch: 117, Train Loss: 7.2080, Val Loss: 7.5849\n",
            "Epoch: 118, Train Loss: 7.1824, Val Loss: 7.5755\n",
            "Epoch: 119, Train Loss: 7.1750, Val Loss: 7.5664\n",
            "Epoch: 120, Train Loss: 7.1541, Val Loss: 7.5575\n",
            "Epoch: 121, Train Loss: 7.1490, Val Loss: 7.5489\n",
            "Epoch: 122, Train Loss: 7.1458, Val Loss: 7.5407\n",
            "Epoch: 123, Train Loss: 7.1230, Val Loss: 7.5327\n",
            "Epoch: 124, Train Loss: 7.1076, Val Loss: 7.5251\n",
            "Epoch: 125, Train Loss: 7.0951, Val Loss: 7.5176\n",
            "Epoch: 126, Train Loss: 7.0874, Val Loss: 7.5103\n",
            "Epoch: 127, Train Loss: 7.0676, Val Loss: 7.5033\n",
            "Epoch: 128, Train Loss: 7.0592, Val Loss: 7.4966\n",
            "Epoch: 129, Train Loss: 7.0482, Val Loss: 7.4900\n",
            "Epoch: 130, Train Loss: 7.0388, Val Loss: 7.4836\n",
            "Epoch: 131, Train Loss: 7.0255, Val Loss: 7.4774\n",
            "Epoch: 132, Train Loss: 7.0226, Val Loss: 7.4716\n",
            "Epoch: 133, Train Loss: 7.0086, Val Loss: 7.4659\n",
            "Epoch: 134, Train Loss: 6.9982, Val Loss: 7.4603\n",
            "Epoch: 135, Train Loss: 6.9892, Val Loss: 7.4550\n",
            "Epoch: 136, Train Loss: 6.9865, Val Loss: 7.4499\n",
            "Epoch: 137, Train Loss: 6.9753, Val Loss: 7.4448\n",
            "Epoch: 138, Train Loss: 6.9594, Val Loss: 7.4400\n",
            "Epoch: 139, Train Loss: 6.9622, Val Loss: 7.4354\n",
            "Epoch: 140, Train Loss: 6.9469, Val Loss: 7.4311\n",
            "Epoch: 141, Train Loss: 6.9340, Val Loss: 7.4267\n",
            "Epoch: 142, Train Loss: 6.9294, Val Loss: 7.4225\n",
            "Epoch: 143, Train Loss: 6.9245, Val Loss: 7.4185\n",
            "Epoch: 144, Train Loss: 6.9116, Val Loss: 7.4147\n",
            "Epoch: 145, Train Loss: 6.9113, Val Loss: 7.4110\n",
            "Epoch: 146, Train Loss: 6.9007, Val Loss: 7.4075\n",
            "Epoch: 147, Train Loss: 6.8941, Val Loss: 7.4041\n",
            "Epoch: 148, Train Loss: 6.8869, Val Loss: 7.4008\n",
            "Epoch: 149, Train Loss: 6.8823, Val Loss: 7.3976\n",
            "Epoch: 150, Train Loss: 6.8754, Val Loss: 7.3945\n",
            "Epoch: 151, Train Loss: 6.8756, Val Loss: 7.3915\n",
            "Epoch: 152, Train Loss: 6.8658, Val Loss: 7.3888\n",
            "Epoch: 153, Train Loss: 6.8574, Val Loss: 7.3860\n",
            "Epoch: 154, Train Loss: 6.8525, Val Loss: 7.3834\n",
            "Epoch: 155, Train Loss: 6.8463, Val Loss: 7.3809\n",
            "Epoch: 156, Train Loss: 6.8396, Val Loss: 7.3785\n",
            "Epoch: 157, Train Loss: 6.8402, Val Loss: 7.3762\n",
            "Epoch: 158, Train Loss: 6.8338, Val Loss: 7.3739\n",
            "Epoch: 159, Train Loss: 6.8286, Val Loss: 7.3718\n",
            "Epoch: 160, Train Loss: 6.8188, Val Loss: 7.3697\n",
            "Epoch: 161, Train Loss: 6.8133, Val Loss: 7.3677\n",
            "Epoch: 162, Train Loss: 6.8089, Val Loss: 7.3657\n",
            "Epoch: 163, Train Loss: 6.8067, Val Loss: 7.3639\n",
            "Epoch: 164, Train Loss: 6.8010, Val Loss: 7.3621\n",
            "Epoch: 165, Train Loss: 6.7997, Val Loss: 7.3605\n",
            "Epoch: 166, Train Loss: 6.7945, Val Loss: 7.3589\n",
            "Epoch: 167, Train Loss: 6.7872, Val Loss: 7.3573\n",
            "Epoch: 168, Train Loss: 6.7908, Val Loss: 7.3558\n",
            "Epoch: 169, Train Loss: 6.7811, Val Loss: 7.3545\n",
            "Epoch: 170, Train Loss: 6.7770, Val Loss: 7.3532\n",
            "Epoch: 171, Train Loss: 6.7739, Val Loss: 7.3518\n",
            "Epoch: 172, Train Loss: 6.7705, Val Loss: 7.3507\n",
            "Epoch: 173, Train Loss: 6.7649, Val Loss: 7.3494\n",
            "Epoch: 174, Train Loss: 6.7603, Val Loss: 7.3483\n",
            "Epoch: 175, Train Loss: 6.7619, Val Loss: 7.3472\n",
            "Epoch: 176, Train Loss: 6.7554, Val Loss: 7.3462\n",
            "Epoch: 177, Train Loss: 6.7516, Val Loss: 7.3452\n",
            "Epoch: 178, Train Loss: 6.7496, Val Loss: 7.3444\n",
            "Epoch: 179, Train Loss: 6.7485, Val Loss: 7.3435\n",
            "Epoch: 180, Train Loss: 6.7450, Val Loss: 7.3427\n",
            "Epoch: 181, Train Loss: 6.7423, Val Loss: 7.3419\n",
            "Epoch: 182, Train Loss: 6.7410, Val Loss: 7.3411\n",
            "Epoch: 183, Train Loss: 6.7358, Val Loss: 7.3405\n",
            "Epoch: 184, Train Loss: 6.7321, Val Loss: 7.3400\n",
            "Epoch: 185, Train Loss: 6.7261, Val Loss: 7.3393\n",
            "Epoch: 186, Train Loss: 6.7283, Val Loss: 7.3388\n",
            "Epoch: 187, Train Loss: 6.7227, Val Loss: 7.3382\n",
            "Epoch: 188, Train Loss: 6.7206, Val Loss: 7.3377\n",
            "Epoch: 189, Train Loss: 6.7215, Val Loss: 7.3373\n",
            "Epoch: 190, Train Loss: 6.7183, Val Loss: 7.3369\n",
            "Epoch: 191, Train Loss: 6.7149, Val Loss: 7.3366\n",
            "Epoch: 192, Train Loss: 6.7110, Val Loss: 7.3363\n",
            "Epoch: 193, Train Loss: 6.7129, Val Loss: 7.3360\n",
            "Epoch: 194, Train Loss: 6.7085, Val Loss: 7.3357\n",
            "Epoch: 195, Train Loss: 6.7047, Val Loss: 7.3353\n",
            "Epoch: 196, Train Loss: 6.7025, Val Loss: 7.3352\n",
            "Epoch: 197, Train Loss: 6.6994, Val Loss: 7.3351\n",
            "Epoch: 198, Train Loss: 6.6981, Val Loss: 7.3349\n",
            "Epoch: 199, Train Loss: 6.6936, Val Loss: 7.3348\n",
            "Epoch: 200, Train Loss: 6.6945, Val Loss: 7.3346\n",
            "Epoch: 201, Train Loss: 6.6932, Val Loss: 7.3346\n",
            "Epoch: 202, Train Loss: 6.6901, Val Loss: 7.3345\n",
            "Epoch: 203, Train Loss: 6.6910, Val Loss: 7.3346\n",
            "Epoch: 204, Train Loss: 6.6869, Val Loss: 7.3347\n",
            "Epoch: 205, Train Loss: 6.6872, Val Loss: 7.3347\n",
            "Epoch: 206, Train Loss: 6.6861, Val Loss: 7.3347\n",
            "Epoch: 207, Train Loss: 6.6814, Val Loss: 7.3348\n",
            "Epoch: 208, Train Loss: 6.6807, Val Loss: 7.3349\n",
            "Epoch: 209, Train Loss: 6.6803, Val Loss: 7.3350\n",
            "Epoch: 210, Train Loss: 6.6773, Val Loss: 7.3351\n",
            "Epoch: 211, Train Loss: 6.6737, Val Loss: 7.3352\n",
            "Epoch: 212, Train Loss: 6.6727, Val Loss: 7.3355\n",
            "Epoch: 213, Train Loss: 6.6734, Val Loss: 7.3357\n",
            "Epoch: 214, Train Loss: 6.6723, Val Loss: 7.3358\n",
            "Epoch: 215, Train Loss: 6.6686, Val Loss: 7.3361\n",
            "Epoch: 216, Train Loss: 6.6679, Val Loss: 7.3364\n",
            "Epoch: 217, Train Loss: 6.6658, Val Loss: 7.3367\n",
            "Epoch: 218, Train Loss: 6.6661, Val Loss: 7.3369\n",
            "Epoch: 219, Train Loss: 6.6660, Val Loss: 7.3373\n",
            "Epoch: 220, Train Loss: 6.6621, Val Loss: 7.3376\n",
            "Epoch: 221, Train Loss: 6.6615, Val Loss: 7.3380\n",
            "Epoch: 222, Train Loss: 6.6603, Val Loss: 7.3383\n",
            "Epoch: 223, Train Loss: 6.6584, Val Loss: 7.3386\n",
            "Epoch: 224, Train Loss: 6.6593, Val Loss: 7.3390\n",
            "Epoch: 225, Train Loss: 6.6558, Val Loss: 7.3394\n",
            "Epoch: 226, Train Loss: 6.6539, Val Loss: 7.3398\n",
            "Epoch: 227, Train Loss: 6.6550, Val Loss: 7.3402\n",
            "Epoch: 228, Train Loss: 6.6522, Val Loss: 7.3407\n",
            "Epoch: 229, Train Loss: 6.6520, Val Loss: 7.3411\n",
            "Epoch: 230, Train Loss: 6.6506, Val Loss: 7.3415\n",
            "Epoch: 231, Train Loss: 6.6491, Val Loss: 7.3420\n",
            "Epoch: 232, Train Loss: 6.6477, Val Loss: 7.3425\n",
            "Epoch: 233, Train Loss: 6.6475, Val Loss: 7.3430\n",
            "Epoch: 234, Train Loss: 6.6473, Val Loss: 7.3436\n",
            "Epoch: 235, Train Loss: 6.6451, Val Loss: 7.3440\n",
            "Epoch: 236, Train Loss: 6.6454, Val Loss: 7.3445\n",
            "Epoch: 237, Train Loss: 6.6439, Val Loss: 7.3450\n",
            "Epoch: 238, Train Loss: 6.6418, Val Loss: 7.3456\n",
            "Epoch: 239, Train Loss: 6.6424, Val Loss: 7.3462\n",
            "Epoch: 240, Train Loss: 6.6410, Val Loss: 7.3467\n",
            "Epoch: 241, Train Loss: 6.6411, Val Loss: 7.3472\n",
            "Epoch: 242, Train Loss: 6.6406, Val Loss: 7.3477\n",
            "Epoch: 243, Train Loss: 6.6382, Val Loss: 7.3484\n",
            "Epoch: 244, Train Loss: 6.6379, Val Loss: 7.3490\n",
            "Epoch: 245, Train Loss: 6.6375, Val Loss: 7.3496\n",
            "Epoch: 246, Train Loss: 6.6359, Val Loss: 7.3501\n",
            "Epoch: 247, Train Loss: 6.6354, Val Loss: 7.3507\n",
            "Epoch: 248, Train Loss: 6.6342, Val Loss: 7.3514\n",
            "Epoch: 249, Train Loss: 6.6334, Val Loss: 7.3520\n",
            "Epoch: 250, Train Loss: 6.6326, Val Loss: 7.3527\n",
            "Epoch: 251, Train Loss: 6.6327, Val Loss: 7.3533\n",
            "Epoch: 252, Train Loss: 6.6332, Val Loss: 7.3539\n",
            "Epoch: 253, Train Loss: 6.6310, Val Loss: 7.3546\n",
            "Epoch: 254, Train Loss: 6.6298, Val Loss: 7.3553\n",
            "Epoch: 255, Train Loss: 6.6297, Val Loss: 7.3559\n",
            "Epoch: 256, Train Loss: 6.6280, Val Loss: 7.3565\n",
            "Epoch: 257, Train Loss: 6.6283, Val Loss: 7.3572\n",
            "Epoch: 258, Train Loss: 6.6277, Val Loss: 7.3578\n",
            "Epoch: 259, Train Loss: 6.6253, Val Loss: 7.3584\n",
            "Epoch: 260, Train Loss: 6.6249, Val Loss: 7.3591\n",
            "Epoch: 261, Train Loss: 6.6263, Val Loss: 7.3598\n",
            "Epoch: 262, Train Loss: 6.6250, Val Loss: 7.3605\n",
            "Epoch: 263, Train Loss: 6.6256, Val Loss: 7.3612\n",
            "Epoch: 264, Train Loss: 6.6233, Val Loss: 7.3619\n",
            "Epoch: 265, Train Loss: 6.6223, Val Loss: 7.3625\n",
            "Epoch: 266, Train Loss: 6.6219, Val Loss: 7.3633\n",
            "Epoch: 267, Train Loss: 6.6215, Val Loss: 7.3639\n",
            "Epoch: 268, Train Loss: 6.6211, Val Loss: 7.3646\n",
            "Epoch: 269, Train Loss: 6.6202, Val Loss: 7.3653\n",
            "Epoch: 270, Train Loss: 6.6197, Val Loss: 7.3660\n",
            "Epoch: 271, Train Loss: 6.6186, Val Loss: 7.3667\n",
            "Epoch: 272, Train Loss: 6.6179, Val Loss: 7.3674\n",
            "Epoch: 273, Train Loss: 6.6184, Val Loss: 7.3681\n",
            "Epoch: 274, Train Loss: 6.6183, Val Loss: 7.3688\n",
            "Epoch: 275, Train Loss: 6.6164, Val Loss: 7.3696\n",
            "Epoch: 276, Train Loss: 6.6168, Val Loss: 7.3702\n",
            "Epoch: 277, Train Loss: 6.6167, Val Loss: 7.3710\n",
            "Epoch: 278, Train Loss: 6.6147, Val Loss: 7.3716\n",
            "Epoch: 279, Train Loss: 6.6152, Val Loss: 7.3723\n",
            "Epoch: 280, Train Loss: 6.6147, Val Loss: 7.3730\n",
            "Epoch: 281, Train Loss: 6.6154, Val Loss: 7.3738\n",
            "Epoch: 282, Train Loss: 6.6136, Val Loss: 7.3745\n",
            "Epoch: 283, Train Loss: 6.6137, Val Loss: 7.3752\n",
            "Epoch: 284, Train Loss: 6.6130, Val Loss: 7.3760\n",
            "Epoch: 285, Train Loss: 6.6122, Val Loss: 7.3766\n",
            "Epoch: 286, Train Loss: 6.6121, Val Loss: 7.3773\n",
            "Epoch: 287, Train Loss: 6.6122, Val Loss: 7.3780\n",
            "Epoch: 288, Train Loss: 6.6114, Val Loss: 7.3787\n",
            "Epoch: 289, Train Loss: 6.6109, Val Loss: 7.3794\n",
            "Epoch: 290, Train Loss: 6.6099, Val Loss: 7.3802\n",
            "Epoch: 291, Train Loss: 6.6101, Val Loss: 7.3809\n",
            "Epoch: 292, Train Loss: 6.6096, Val Loss: 7.3817\n",
            "Epoch: 293, Train Loss: 6.6092, Val Loss: 7.3825\n",
            "Epoch: 294, Train Loss: 6.6090, Val Loss: 7.3831\n",
            "Epoch: 295, Train Loss: 6.6084, Val Loss: 7.3838\n",
            "Epoch: 296, Train Loss: 6.6075, Val Loss: 7.3846\n",
            "Epoch: 297, Train Loss: 6.6078, Val Loss: 7.3853\n",
            "Epoch: 298, Train Loss: 6.6065, Val Loss: 7.3860\n",
            "Epoch: 299, Train Loss: 6.6058, Val Loss: 7.3868\n",
            "Epoch: 300, Train Loss: 6.6056, Val Loss: 7.3874\n",
            "Epoch: 301, Train Loss: 6.6059, Val Loss: 7.3882\n",
            "Epoch: 302, Train Loss: 6.6059, Val Loss: 7.3889\n",
            "Epoch: 303, Train Loss: 6.6049, Val Loss: 7.3896\n",
            "Epoch: 304, Train Loss: 6.6049, Val Loss: 7.3899\n",
            "Epoch: 305, Train Loss: 6.6025, Val Loss: 7.3907\n",
            "Epoch: 306, Train Loss: 6.6035, Val Loss: 7.3915\n",
            "Epoch: 307, Train Loss: 6.6044, Val Loss: 7.3920\n",
            "Epoch: 308, Train Loss: 6.6039, Val Loss: 7.3927\n",
            "Epoch: 309, Train Loss: 6.6018, Val Loss: 7.3935\n",
            "Epoch: 310, Train Loss: 6.6028, Val Loss: 7.3939\n",
            "Epoch: 311, Train Loss: 6.6017, Val Loss: 7.3947\n",
            "Epoch: 312, Train Loss: 6.6026, Val Loss: 7.3955\n",
            "Epoch: 313, Train Loss: 6.6013, Val Loss: 7.3962\n",
            "Epoch: 314, Train Loss: 6.6017, Val Loss: 7.3969\n",
            "Epoch: 315, Train Loss: 6.6015, Val Loss: 7.3978\n",
            "Epoch: 316, Train Loss: 6.6006, Val Loss: 7.3986\n",
            "Epoch: 317, Train Loss: 6.6001, Val Loss: 7.3993\n",
            "Epoch: 318, Train Loss: 6.6003, Val Loss: 7.4000\n",
            "Epoch: 319, Train Loss: 6.6003, Val Loss: 7.4008\n",
            "Epoch: 320, Train Loss: 6.5983, Val Loss: 7.4016\n",
            "Epoch: 321, Train Loss: 6.5989, Val Loss: 7.4022\n",
            "Epoch: 322, Train Loss: 6.5995, Val Loss: 7.4029\n",
            "Epoch: 323, Train Loss: 6.5990, Val Loss: 7.4036\n",
            "Epoch: 324, Train Loss: 6.5983, Val Loss: 7.4044\n",
            "Epoch: 325, Train Loss: 6.5974, Val Loss: 7.4052\n",
            "Epoch: 326, Train Loss: 6.5982, Val Loss: 7.4058\n",
            "Epoch: 327, Train Loss: 6.5971, Val Loss: 7.4066\n",
            "Epoch: 328, Train Loss: 6.5971, Val Loss: 7.4073\n",
            "Epoch: 329, Train Loss: 6.5969, Val Loss: 7.4081\n",
            "Epoch: 330, Train Loss: 6.5960, Val Loss: 7.4088\n",
            "Epoch: 331, Train Loss: 6.5963, Val Loss: 7.4095\n",
            "Epoch: 332, Train Loss: 6.5962, Val Loss: 7.4102\n",
            "Epoch: 333, Train Loss: 6.5963, Val Loss: 7.4108\n",
            "Epoch: 334, Train Loss: 6.5954, Val Loss: 7.4115\n",
            "Epoch: 335, Train Loss: 6.5945, Val Loss: 7.4123\n",
            "Epoch: 336, Train Loss: 6.5953, Val Loss: 7.4132\n",
            "Epoch: 337, Train Loss: 6.5937, Val Loss: 7.4140\n",
            "Epoch: 338, Train Loss: 6.5940, Val Loss: 7.4147\n",
            "Epoch: 339, Train Loss: 6.5931, Val Loss: 7.4154\n",
            "Epoch: 340, Train Loss: 6.5928, Val Loss: 7.4161\n",
            "Epoch: 341, Train Loss: 6.5930, Val Loss: 7.4168\n",
            "Epoch: 342, Train Loss: 6.5929, Val Loss: 7.4175\n",
            "Epoch: 343, Train Loss: 6.5923, Val Loss: 7.4182\n",
            "Epoch: 344, Train Loss: 6.5931, Val Loss: 7.4188\n",
            "Epoch: 345, Train Loss: 6.5916, Val Loss: 7.4195\n",
            "Epoch: 346, Train Loss: 6.5924, Val Loss: 7.4201\n",
            "Epoch: 347, Train Loss: 6.5920, Val Loss: 7.4209\n",
            "Epoch: 348, Train Loss: 6.5913, Val Loss: 7.4216\n",
            "Epoch: 349, Train Loss: 6.5902, Val Loss: 7.4222\n",
            "Epoch: 350, Train Loss: 6.5905, Val Loss: 7.4229\n",
            "Epoch: 351, Train Loss: 6.5908, Val Loss: 7.4231\n",
            "Epoch: 352, Train Loss: 6.5909, Val Loss: 7.4239\n",
            "Epoch: 353, Train Loss: 6.5902, Val Loss: 7.4246\n",
            "Epoch: 354, Train Loss: 6.5898, Val Loss: 7.4253\n",
            "Epoch: 355, Train Loss: 6.5892, Val Loss: 7.4261\n",
            "Epoch: 356, Train Loss: 6.5898, Val Loss: 7.4267\n",
            "Epoch: 357, Train Loss: 6.5887, Val Loss: 7.4274\n",
            "Epoch: 358, Train Loss: 6.5888, Val Loss: 7.4280\n",
            "Epoch: 359, Train Loss: 6.5881, Val Loss: 7.4287\n",
            "Epoch: 360, Train Loss: 6.5875, Val Loss: 7.4294\n",
            "Epoch: 361, Train Loss: 6.5884, Val Loss: 7.4301\n",
            "Epoch: 362, Train Loss: 6.5882, Val Loss: 7.4307\n",
            "Epoch: 363, Train Loss: 6.5872, Val Loss: 7.4315\n",
            "Epoch: 364, Train Loss: 6.5877, Val Loss: 7.4322\n",
            "Epoch: 365, Train Loss: 6.5871, Val Loss: 7.4328\n",
            "Epoch: 366, Train Loss: 6.5876, Val Loss: 7.4335\n",
            "Epoch: 367, Train Loss: 6.5865, Val Loss: 7.4341\n",
            "Epoch: 368, Train Loss: 6.5858, Val Loss: 7.4348\n",
            "Epoch: 369, Train Loss: 6.5867, Val Loss: 7.4355\n",
            "Epoch: 370, Train Loss: 6.5865, Val Loss: 7.4362\n",
            "Epoch: 371, Train Loss: 6.5861, Val Loss: 7.4368\n",
            "Epoch: 372, Train Loss: 6.5858, Val Loss: 7.4375\n",
            "Epoch: 373, Train Loss: 6.5855, Val Loss: 7.4383\n",
            "Epoch: 374, Train Loss: 6.5858, Val Loss: 7.4389\n",
            "Epoch: 375, Train Loss: 6.5850, Val Loss: 7.4395\n",
            "Epoch: 376, Train Loss: 6.5843, Val Loss: 7.4401\n",
            "Epoch: 377, Train Loss: 6.5839, Val Loss: 7.4408\n",
            "Epoch: 378, Train Loss: 6.5845, Val Loss: 7.4415\n",
            "Epoch: 379, Train Loss: 6.5839, Val Loss: 7.4422\n",
            "Epoch: 380, Train Loss: 6.5836, Val Loss: 7.4427\n",
            "Epoch: 381, Train Loss: 6.5835, Val Loss: 7.4435\n",
            "Epoch: 382, Train Loss: 6.5825, Val Loss: 7.4440\n",
            "Epoch: 383, Train Loss: 6.5827, Val Loss: 7.4446\n",
            "Epoch: 384, Train Loss: 6.5826, Val Loss: 7.4453\n",
            "Epoch: 385, Train Loss: 6.5830, Val Loss: 7.4459\n",
            "Epoch: 386, Train Loss: 6.5825, Val Loss: 7.4466\n",
            "Epoch: 387, Train Loss: 6.5827, Val Loss: 7.4471\n",
            "Epoch: 388, Train Loss: 6.5819, Val Loss: 7.4479\n",
            "Epoch: 389, Train Loss: 6.5814, Val Loss: 7.4485\n",
            "Epoch: 390, Train Loss: 6.5814, Val Loss: 7.4491\n",
            "Epoch: 391, Train Loss: 6.5817, Val Loss: 7.4498\n",
            "Epoch: 392, Train Loss: 6.5814, Val Loss: 7.4505\n",
            "Epoch: 393, Train Loss: 6.5802, Val Loss: 7.4511\n",
            "Epoch: 394, Train Loss: 6.5804, Val Loss: 7.4517\n",
            "Epoch: 395, Train Loss: 6.5811, Val Loss: 7.4524\n",
            "Epoch: 396, Train Loss: 6.5805, Val Loss: 7.4530\n",
            "Epoch: 397, Train Loss: 6.5797, Val Loss: 7.4536\n",
            "Epoch: 398, Train Loss: 6.5803, Val Loss: 7.4543\n",
            "Epoch: 399, Train Loss: 6.5800, Val Loss: 7.4549\n",
            "Epoch: 400, Train Loss: 6.5796, Val Loss: 7.4555\n",
            "Epoch: 401, Train Loss: 6.5794, Val Loss: 7.4562\n",
            "Epoch: 402, Train Loss: 6.5784, Val Loss: 7.4568\n",
            "Epoch: 403, Train Loss: 6.5797, Val Loss: 7.4574\n",
            "Epoch: 404, Train Loss: 6.5787, Val Loss: 7.4580\n",
            "Epoch: 405, Train Loss: 6.5787, Val Loss: 7.4587\n",
            "Epoch: 406, Train Loss: 6.5780, Val Loss: 7.4593\n",
            "Epoch: 407, Train Loss: 6.5774, Val Loss: 7.4600\n",
            "Epoch: 408, Train Loss: 6.5779, Val Loss: 7.4606\n",
            "Epoch: 409, Train Loss: 6.5779, Val Loss: 7.4612\n",
            "Epoch: 410, Train Loss: 6.5776, Val Loss: 7.4618\n",
            "Epoch: 411, Train Loss: 6.5771, Val Loss: 7.4623\n",
            "Epoch: 412, Train Loss: 6.5774, Val Loss: 7.4629\n",
            "Epoch: 413, Train Loss: 6.5766, Val Loss: 7.4636\n",
            "Epoch: 414, Train Loss: 6.5763, Val Loss: 7.4640\n",
            "Epoch: 415, Train Loss: 6.5762, Val Loss: 7.4647\n",
            "Epoch: 416, Train Loss: 6.5765, Val Loss: 7.4654\n",
            "Epoch: 417, Train Loss: 6.5761, Val Loss: 7.4660\n",
            "Epoch: 418, Train Loss: 6.5763, Val Loss: 7.4665\n",
            "Epoch: 419, Train Loss: 6.5759, Val Loss: 7.4672\n",
            "Epoch: 420, Train Loss: 6.5753, Val Loss: 7.4680\n",
            "Epoch: 421, Train Loss: 6.5751, Val Loss: 7.4683\n",
            "Epoch: 422, Train Loss: 6.5755, Val Loss: 7.4689\n",
            "Epoch: 423, Train Loss: 6.5750, Val Loss: 7.4699\n",
            "Epoch: 424, Train Loss: 6.5749, Val Loss: 7.4703\n",
            "Epoch: 425, Train Loss: 6.5748, Val Loss: 7.4708\n",
            "Epoch: 426, Train Loss: 6.5745, Val Loss: 7.4716\n",
            "Epoch: 427, Train Loss: 6.5747, Val Loss: 7.4722\n",
            "Epoch: 428, Train Loss: 6.5744, Val Loss: 7.4727\n",
            "Epoch: 429, Train Loss: 6.5743, Val Loss: 7.4731\n",
            "Epoch: 430, Train Loss: 6.5736, Val Loss: 7.4740\n",
            "Epoch: 431, Train Loss: 6.5735, Val Loss: 7.4746\n",
            "Epoch: 432, Train Loss: 6.5734, Val Loss: 7.4751\n",
            "Epoch: 433, Train Loss: 6.5740, Val Loss: 7.4756\n",
            "Epoch: 434, Train Loss: 6.5733, Val Loss: 7.4763\n",
            "Epoch: 435, Train Loss: 6.5724, Val Loss: 7.4772\n",
            "Epoch: 436, Train Loss: 6.5725, Val Loss: 7.4775\n",
            "Epoch: 437, Train Loss: 6.5727, Val Loss: 7.4781\n",
            "Epoch: 438, Train Loss: 6.5723, Val Loss: 7.4790\n",
            "Epoch: 439, Train Loss: 6.5719, Val Loss: 7.4796\n",
            "Epoch: 440, Train Loss: 6.5725, Val Loss: 7.4797\n",
            "Epoch: 441, Train Loss: 6.5718, Val Loss: 7.4803\n",
            "Epoch: 442, Train Loss: 6.5714, Val Loss: 7.4812\n",
            "Epoch: 443, Train Loss: 6.5716, Val Loss: 7.4816\n",
            "Epoch: 444, Train Loss: 6.5714, Val Loss: 7.4821\n",
            "Epoch: 445, Train Loss: 6.5715, Val Loss: 7.4830\n",
            "Epoch: 446, Train Loss: 6.5710, Val Loss: 7.4834\n",
            "Epoch: 447, Train Loss: 6.5705, Val Loss: 7.4840\n",
            "Epoch: 448, Train Loss: 6.5700, Val Loss: 7.4849\n",
            "Epoch: 449, Train Loss: 6.5702, Val Loss: 7.4852\n",
            "Epoch: 450, Train Loss: 6.5699, Val Loss: 7.4859\n",
            "Epoch: 451, Train Loss: 6.5707, Val Loss: 7.4866\n",
            "Epoch: 452, Train Loss: 6.5699, Val Loss: 7.4870\n",
            "Epoch: 453, Train Loss: 6.5693, Val Loss: 7.4877\n",
            "Epoch: 454, Train Loss: 6.5697, Val Loss: 7.4880\n",
            "Epoch: 455, Train Loss: 6.5694, Val Loss: 7.4887\n",
            "Epoch: 456, Train Loss: 6.5693, Val Loss: 7.4892\n",
            "Epoch: 457, Train Loss: 6.5688, Val Loss: 7.4897\n",
            "Epoch: 458, Train Loss: 6.5685, Val Loss: 7.4903\n",
            "Epoch: 459, Train Loss: 6.5694, Val Loss: 7.4910\n",
            "Epoch: 460, Train Loss: 6.5687, Val Loss: 7.4914\n",
            "Epoch: 461, Train Loss: 6.5684, Val Loss: 7.4921\n",
            "Epoch: 462, Train Loss: 6.5684, Val Loss: 7.4926\n",
            "Epoch: 463, Train Loss: 6.5688, Val Loss: 7.4933\n",
            "Epoch: 464, Train Loss: 6.5680, Val Loss: 7.4939\n",
            "Epoch: 465, Train Loss: 6.5678, Val Loss: 7.4946\n",
            "Epoch: 466, Train Loss: 6.5676, Val Loss: 7.4951\n",
            "Epoch: 467, Train Loss: 6.5675, Val Loss: 7.4957\n",
            "Epoch: 468, Train Loss: 6.5671, Val Loss: 7.4963\n",
            "Epoch: 469, Train Loss: 6.5672, Val Loss: 7.4969\n",
            "Epoch: 470, Train Loss: 6.5668, Val Loss: 7.4974\n",
            "Epoch: 471, Train Loss: 6.5669, Val Loss: 7.4981\n",
            "Epoch: 472, Train Loss: 6.5666, Val Loss: 7.4987\n",
            "Epoch: 473, Train Loss: 6.5659, Val Loss: 7.4993\n",
            "Epoch: 474, Train Loss: 6.5661, Val Loss: 7.4999\n",
            "Epoch: 475, Train Loss: 6.5662, Val Loss: 7.5004\n",
            "Epoch: 476, Train Loss: 6.5655, Val Loss: 7.5011\n",
            "Epoch: 477, Train Loss: 6.5656, Val Loss: 7.5016\n",
            "Epoch: 478, Train Loss: 6.5658, Val Loss: 7.5022\n",
            "Epoch: 479, Train Loss: 6.5650, Val Loss: 7.5029\n",
            "Epoch: 480, Train Loss: 6.5651, Val Loss: 7.5035\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-9c6d0c2e19d6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-c734722fc8d4>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, criterion)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-cb2ac5831702>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtarget_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING THE GENERATION SUMMARIZATION"
      ],
      "metadata": {
        "id": "QCOL1a2PmD80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_summary(\n",
        "    model,\n",
        "    article,\n",
        "    vocab,\n",
        "    max_summary_length,\n",
        "    temperature=1.0,\n",
        "    top_k=5,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2\n",
        "):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Prepare the input article\n",
        "        article = torch.tensor(article).unsqueeze(0).to(model.device)\n",
        "        hidden, cell = model.encoder(article)\n",
        "\n",
        "        # Initialize the decoding process\n",
        "        input = torch.tensor([vocab.word2idx['<sos>']]).unsqueeze(0).to(model.device)\n",
        "        summary = []\n",
        "        token_counts = {}  # Track token frequencies to penalize repetitions\n",
        "\n",
        "        for step in range(max_summary_length):\n",
        "            # Decoder forward pass\n",
        "            output, hidden, cell = model.decoder(input, hidden, cell)\n",
        "\n",
        "            # Extract logits for the current token\n",
        "            logits = output[0, 0, :] / temperature  # Scale logits with temperature\n",
        "            probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Apply repetition penalty\n",
        "            for token_idx in summary:\n",
        "                probabilities[token_idx] /= repetition_penalty\n",
        "\n",
        "            # Top-p sampling (nucleus sampling)\n",
        "            sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            cutoff_idx = (cumulative_probs > top_p).nonzero(as_tuple=True)[0][0]\n",
        "            top_p_probs = sorted_probs[:cutoff_idx + 1]\n",
        "            top_p_indices = sorted_indices[:cutoff_idx + 1]\n",
        "            top_p_probs = top_p_probs / top_p_probs.sum()  # Renormalize\n",
        "            predicted_word_index = torch.multinomial(top_p_probs, 1).item()\n",
        "\n",
        "            # Debugging: Log probabilities and predictions\n",
        "            print(f\"Step {step}:\")\n",
        "            print(f\"  Probabilities (top 10): {sorted_probs[:10]}\")\n",
        "            print(f\"  Predicted Word Index: {predicted_word_index} ({vocab.idx2word[predicted_word_index]})\")\n",
        "\n",
        "            # Append the predicted token to the summary\n",
        "            summary.append(predicted_word_index)\n",
        "            token_counts[predicted_word_index] = token_counts.get(predicted_word_index, 0) + 1\n",
        "\n",
        "            # Update input for the next step\n",
        "            input = torch.tensor([predicted_word_index]).unsqueeze(0).to(model.device)\n",
        "\n",
        "            # Stop decoding if <eos> token is predicted\n",
        "            if predicted_word_index == vocab.word2idx['<eos>']:\n",
        "                break\n",
        "\n",
        "        # Convert word indices back to words\n",
        "        summary = [vocab.idx2word[idx] for idx in summary]\n",
        "        return ' '.join(summary)\n",
        "\n",
        "# Example inference\n",
        "article = \"kamu ini kok gitu?\"\n",
        "article_tokens = tokenize(article, vocab, max_article_length)\n",
        "\n",
        "# Generate summary\n",
        "summary = generate_summary(\n",
        "    model,\n",
        "    article_tokens,\n",
        "    vocab,\n",
        "    max_summary_length=50,\n",
        "    temperature=0.7,\n",
        "    top_k=5,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "print(f\"Summary: {summary}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBxa267vmy6g",
        "outputId": "d97f8de7-0417-472b-e589-39db158b4c67"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0:\n",
            "  Probabilities (top 10): tensor([0.0455, 0.0398, 0.0381, 0.0295, 0.0283, 0.0277, 0.0275, 0.0244, 0.0187,\n",
            "        0.0139], device='cuda:0')\n",
            "  Predicted Word Index: 3 (<unk>)\n",
            "Step 1:\n",
            "  Probabilities (top 10): tensor([0.0525, 0.0410, 0.0384, 0.0336, 0.0282, 0.0279, 0.0254, 0.0252, 0.0233,\n",
            "        0.0180], device='cuda:0')\n",
            "  Predicted Word Index: 17 (dapat)\n",
            "Step 2:\n",
            "  Probabilities (top 10): tensor([0.0744, 0.0487, 0.0383, 0.0337, 0.0274, 0.0268, 0.0233, 0.0217, 0.0211,\n",
            "        0.0208], device='cuda:0')\n",
            "  Predicted Word Index: 344 (mencari)\n",
            "Step 3:\n",
            "  Probabilities (top 10): tensor([0.0994, 0.0459, 0.0456, 0.0300, 0.0273, 0.0243, 0.0242, 0.0205, 0.0191,\n",
            "        0.0190], device='cuda:0')\n",
            "  Predicted Word Index: 21 (secara)\n",
            "Step 4:\n",
            "  Probabilities (top 10): tensor([0.1117, 0.0530, 0.0437, 0.0359, 0.0273, 0.0244, 0.0229, 0.0208, 0.0204,\n",
            "        0.0170], device='cuda:0')\n",
            "  Predicted Word Index: 98 (sebuah)\n",
            "Step 5:\n",
            "  Probabilities (top 10): tensor([0.1046, 0.0566, 0.0380, 0.0367, 0.0295, 0.0264, 0.0226, 0.0222, 0.0172,\n",
            "        0.0161], device='cuda:0')\n",
            "  Predicted Word Index: 17 (dapat)\n",
            "Step 6:\n",
            "  Probabilities (top 10): tensor([0.1240, 0.0543, 0.0386, 0.0370, 0.0282, 0.0259, 0.0243, 0.0239, 0.0167,\n",
            "        0.0146], device='cuda:0')\n",
            "  Predicted Word Index: 1 (<sos>)\n",
            "Step 7:\n",
            "  Probabilities (top 10): tensor([0.1335, 0.0594, 0.0441, 0.0344, 0.0290, 0.0283, 0.0242, 0.0229, 0.0176,\n",
            "        0.0159], device='cuda:0')\n",
            "  Predicted Word Index: 2 (<eos>)\n",
            "Summary: <unk> dapat mencari secara sebuah dapat <sos> <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6HGN6P8m0eI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}